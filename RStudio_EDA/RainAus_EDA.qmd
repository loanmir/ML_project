---
title: "Machine Learning Project 2024"
author: 
    - name: "Lucas Jakin"
    - name: "Saša Nanut"
    - name: "Luca Marega"
date: "2024-06-04"
output: html_document
editor: visual
format: 
  html: 
    toc: true
    toc-location: right
    embed-resources: true
execute: 
  echo: true
  error: true
  warning: false
  messaeges: false
---

# Predicting next-day rain in Australia

#### Introduction

As a group we decided to take on the **first project type**. The project focuses on utilizing DM & ML algorithms to address a specififc problem chosen from Kaggle.The Goal of the project is to address the classification problem by utilizing more than one classification algorithm, in order to do a systematic experimentation with different algorithms to identify in what they differ and which one is the most effective one for the chosen dataset. We will consider different classification algorithms and make the comparison between three of them, more precisely *Artificial Neural Networks*, *CatBoost* and *Logistic Regression*.\

We will divide the work as the following: as a group we will perform a brief analysis of the dataset and make some cleaning of it if needed. Afterwards, each one of us will implement one of the previously mentioned algorithms and then we will compare and interpret the results

## About the dataset

The dataset found in [Kaggle](https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package/code) consists of about 10 years of daily weather observations from numerous locations across Australia.\

The problem that is required to be solved from this dataset represents a classification problem, in this case a **binary classification** problem. The objective is to predict whether it will rain tomorrow or not with high accuracy. The dataframe contains 145460 observations (rows) and 23 attributes. The observations are weather conditions of days of a specific region including: date, location, minimum and maximum temperature, rain fall, humidity and so on.\

The most important feature of the dataset is the last column **"RainTomorrow"**, which is the target variable for our ML models that we want to predict.

It has two values:

-   Yes –\> It will rain tomorrow

-   No –\> It will not rain tomorrow.

## Exploratory Data Analysis

```{r}
library(tidyverse)
library(dplyr)
library(skimr)
library(ggcorrplot)
library(gt)
library(ggplot2)
weatherAus <- read.csv("weatherAUS.csv", header = T)
```

As we start, we first load the weather data and look at the first rows to identify the features:

```{r}
head(weatherAus) %>% gt()
```

\
In the next step we check out the summary statistics of the dataset and identify the numerical and categorical attributes:

```{r}
skim(weatherAus)
```

As we can see from the figure above, there are 7 **categorical** attributes and 16 **numerical** attributes.\
Before taking a deeper look on all other attributes, we first did a brief exploration of the target variable:

-   **MISSING VALUES**

```{r}
missingValues <- sum(is.na(weatherAus$RainTomorrow))
missingValues
```

-   **FREQUENCY DISTRIBUTION OF VALUES**

```{r}
  weatherAus %>% select(RainTomorrow) %>%
  count(RainTomorrow) %>% drop_na() %>%
  ggplot(., aes(RainTomorrow, n, fill=RainTomorrow)) +
  geom_col(width = 0.5)+
  labs(x = "RainTomorrow", y = "Count")+
  theme_minimal()
```

-   **RATIO OF FREQUENCY DISTRIBUTION**

```{r}
  weatherAus %>% select(RainTomorrow) %>%
  count(RainTomorrow) %>% drop_na() %>%
  ggplot(., aes(x="", n, fill = RainTomorrow)) +
  geom_bar(width = 1, size = 1, color = "white",stat = "identity") + coord_polar("y", start = 0) +
  geom_text(aes(label = paste0(round((n/145460)*100),"%")),
            position = position_stack(vjust = 0.5)) +
  theme_classic() +
  labs(x = NULL, y = NULL) +
  theme(axis.line = element_blank())
```

\
From the plots drawn above, we can clearly see that RainTomorrow has 2 categories of values: **Yes** and **No**. There are far more NEGATIVE values than POSITIVE.

```{r}
  weatherAus %>% select(RainToday) %>%
  count(RainToday) %>% drop_na() %>%
  ggplot(., aes(x="", n, fill = RainToday)) +
  geom_bar(width = 1, size = 1, color = "white",stat = "identity") + coord_polar("y", start = 0) +
  geom_text(aes(label = paste0(round((n/145460)*100),"%")),
            position = position_stack(vjust = 0.5)) +
  theme_classic() +
  labs(x = NULL, y = NULL) +
  theme(axis.line = element_blank())
```

\

The variable **RainToday** has a very similar value distribution as the target variable. ...!!!!!!!!!!!!!!!!!!!!!

------------------------------------------------------------------------

### Categorical values

All together there are 6 categorical features + a Date column. In order to make the information about the date more useful, we decided to extract the year, the month and the day from the date into three separate columns.\
This is done here below:

```{r}
weatherAusNew <- weatherAus %>% mutate(
  Year = year(Date),
  Month = month(Date),
  Day = day(Date)
) %>% select(-Date)
as_tibble(weatherAusNew)
```

------------------------------------------------------------------------

### Numerical values

There are 16 numerical attributes in the raw dataset, after adding three columns for year, month and day, there are in total 19 numerical attributes. The main goal when analyzing numerical data is to find the outliers. Outliers are data information that differ significantly from other observations.

The most efficient way to detect outliers is to draw box plots:

> MinTemp

```{r}
 boxplot(weatherAusNew$MinTemp, col = "blue", border = "black")
```

> MaxTemp

```{r}
 boxplot(weatherAusNew$MaxTemp, col = "blue", border = "black")
```

> Rainfall

```{r}
 boxplot(weatherAusNew$Rainfall, col = "blue", border = "black")
```

> Evaporation

```{r}
 boxplot(weatherAusNew$Evaporation, col = "blue", border = "black")
```

> Sunshine

```{r}
 boxplot(weatherAusNew$Sunshine, col = "blue", border = "black")
```

> WindGustSpeed

```{r}
 boxplot(weatherAusNew$WindGustSpeed, col = "blue", border = "black")
```

> WindSpeed9am

```{r}
 boxplot(weatherAusNew$WindSpeed9am, col = "blue", border = "black")
```

> WindSpeed3pm

```{r}
 boxplot(weatherAusNew$WindSpeed3pm, col = "blue", border = "black")
```

> Humidity9am

```{r}
 boxplot(weatherAusNew$Humidity9am, col = "blue", border = "black")
```

> Humidity3pm

```{r}
 boxplot(weatherAusNew$Humidity3pm, col = "blue", border = "black")
```

> Pressure9am

```{r}
 boxplot(weatherAusNew$Pressure9am, col = "blue", border = "black")
```

> Pressure3pm

```{r}
 boxplot(weatherAusNew$Pressure3pm, col = "blue", border = "black")
```

> Cloud9am

```{r}
 boxplot(weatherAusNew$Cloud9am, col = "blue", border = "black")
```

> Cloud3pm

```{r}
 boxplot(weatherAusNew$Cloud3pm, col = "blue", border = "black")
```

> Temp9am

```{r}
 boxplot(weatherAusNew$Temp9am, col = "blue", border = "black")
```

> Temp3pm

```{r}
 boxplot(weatherAusNew$Temp3pm, col = "blue", border = "black")
```

> Year

```{r}
 boxplot(weatherAusNew$Year, col = "blue", border = "black")
```

> Month

```{r}
 boxplot(weatherAusNew$Month, col = "blue", border = "black")
```

> Day

```{r}
 boxplot(weatherAusNew$Day, col = "blue", border = "black")
```

#### Outliers

After drawing a boxplot for each numerical attribute in the dataset, we compared the mean of each column with the min/max value and we have noticed that that attributes **Rainfall**, **Evaporation**, **WindSpeed9am** and **WindSpeed3pm** might have a large number of outliers as there as there's a considerable difference between average value and max value. This also can be seen from their plots, as there is a huge amount of points (values) that differ from the average.

Let's remove the outliers values from the dataset:

```{r}

```

### Multicollinearity

```{r}
#| label: fig-absences
#| fig-cap: "Correlation Heatmap"
weatherAusNew %>% select(where(is.numeric)) %>% model.matrix(~0+.,
              data=.) %>%
    cor(use="pairwise.complete.obs") %>% 
    ggcorrplot(show.diag = FALSE, type="full",
               lab=TRUE,legend.title = "Correlation" ,lab_size
               = 2,lab_col = "black" ,ggtheme =
                 ggplot2::theme_gray,
               colors = c("white","green","darkgreen"),
               outline.color = "black")
```

------------------------------------------------------------------------

MAYBE SET THE **RAINTOMORROW** VALUES yes/no -\> 1/0!!!!! CHECK

look at function for deleting outliers, more analysis before modeling

## Modeling

### Artificial Neural Network

-   The ANN model derives from Biological neural networks that have the structure of the human brain. It contains neurons(nodes) interconnected to one another in various layers of the network. It consists of three layers: Input layer, Hidden layers (can be several of them) and Output layer. The input layer accepts inputs in several formats, the hidden layer is in-between the inputs and outputs and performs calculations to find hidden features and patterns. The output layer outputs the results of calculations.

    ![ANN layers](ANNPicture.png)
    
\


-   *ADVANTAGES*

    -   **Parallel processing**

    -   Information can produce **output** even with **inadequate data**

    -   **Success** proportional to **chosen instances**

-   *DISADVANTAGES*

    -   Depends on **hardware**

    -   ANNs work with **numerical data**

    -   Duration of network is **unknown**

-   **FUNCTIONING**

    Each input is multiplied by its corresponding weights (strength of interconnections between neurons). All weighted inputs are summarized inside the computing unit. Each neuron has its **bias,** which is added to the weighted sum to make it non-zero, so the total sum of weighted inputs can be from 0 to plus infinity. The maximum value is **benchmarked** to keep the response in the limits. This is performed in *TRANSFER FUNCTIONS.*

    *ACTIVATION FUNCTIONS* choose whether a node should fire or not. Only those who are *fired* make it to the output layer. Activation functions are distinctive depending on the task that is performed.

-   **FEED-BACK**

    -   Feed-back networks feed information back to itself

-   **FEED-FORWARD**

    -   Assessment of ouputs by reviewing its inputs

    -   Input –\> Neuron layer –\> Output

------------------------------------------------------------------------

```{python}


# Python3 code to demonstrate
# matrix creation of n * n
# using list comprehension
 
# initializing N
N = 4
 
# printing dimension
print("The dimension : " + str(N))
 
# using list comprehension
# matrix creation of n * n
res = [list(range(1 + N * i, 1 + N * (i + 1)))
                            for i in range(N)]
 
# print result
print("The created matrix of N * N: " + str(res))
```

### CatBoost

-   **Categorical Boosting** is an open-source boosting library designed for use on problems like regression and classification having a very large number of independent features. Ii is a variant of gradient boosting that can handle both categorical and numerical features. It also uses an algorithm called **symmetric weighted quantile sketch (SWQS)** which automatically handles the missing values in the dataset to reduce overfitting and improve the overall performance of the dataset.

------------------------------------------------------------------------

```{python}

```

### Logistic Regression

-   **Logistic Regression** is a supervised machine learning algorithm used for classification tasks where the goal is to predict the probability that an instance belongs to a given class or not. This model is used for binary classification where we use the sigmoid function, that takes input as independent variables and produces a probability value between 0 and 1.

-   **Key Concepts:**

    -   Logistic Regression predicts the output of a categorical dependent variable. In our case we have the target variable *"RainTomorrow"* that has values "yes" and "no".

    -   It gives the probabilistic values which is between 0 and 1.

    -   Instead of fitting a regression line, we fit an **S shaped** logistic function, which predicts two maximum values.

        ![Sigmoid Function](LogRegPicture.png)
        
\

-   [**Sigmoid function**]{style="color:red;"} is a mathematical function used to map the predicted values to probabilities. It maps any real value into another value within the range of 0 and 1. This range is called the **Threshold value**.

-   **Types of Logistic regression**

    -   [Binomial:]{.underline} there can be only two possible types of dependent variables, such as 0 and 1, Pass or Fail, etc.

    -   [Multinomial:]{.underline} there can be 3 or more possible unordered types of the dependent variable, such as "cat", "dogs", or "sheep".

    -   [Ordinal:]{.underline} there can be 3 or more possible ordered types of dependent variables, such as "low", "medium", "high".

```{python}

```
