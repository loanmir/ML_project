---
title: "Machine Learning Project 2024"
author: 
    - name: "Lucas Jakin"
    - name: "Saša Nanut"
    - name: "Luca Marega"
date: "2024-06-04"
output: html_document
editor: visual
format: 
  html: 
    toc: true
    toc-location: right
    embed-resources: true
execute: 
  echo: true
  error: true
  warning: false
  messaeges: false
---

# Predicting next-day rain in Australia

#### Introduction

As a group we decided to take on the **first project type**. The project focuses on utilizing DM & ML algorithms to address a specific problem chosen from Kaggle.The Goal of the project is to address the classification problem by utilizing more than one classification algorithm, in order to do a systematic experimentation with different algorithms to identify in what they differ and which one is the most effective one for the chosen dataset. We will consider different classification algorithms and make the comparison between three of them, more precisely *Artificial Neural Networks*, *CatBoost* and *Logistic Regression*.\

We will divide the work as the following: as a group we will perform a brief analysis of the dataset and make some cleaning of it if needed. Afterwards, each one of us will implement one of the previously mentioned algorithms and then we will compare and interpret the results

## About the dataset

The dataset found in [Kaggle](https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package/code) consists of about 10 years of daily weather observations from numerous locations across Australia.\

The problem that is required to be solved from this dataset represents a classification problem, in this case a **binary classification** problem. The objective is to predict whether it will rain tomorrow or not with high accuracy. The dataframe contains 145460 observations (rows) and 23 attributes. The observations are weather conditions of days of a specific region including: date, location, minimum and maximum temperature, rain fall, humidity and so on.\

The most important feature of the dataset is the last column **"RainTomorrow"**, which is the target variable for our ML models that we want to predict.

It has two values:

-   Yes –\> It will rain tomorrow

-   No –\> It will not rain tomorrow.

## Exploratory Data Analysis

```{r}
library(tidyverse)
library(dplyr)
library(skimr)
library(ggcorrplot)
library(gt)
library(ggplot2)

#use_python("C:\\Python312\python.exe", required = T)
weatherAus <- read.csv("weatherAUS.csv", header = T)
```



As we start, we first load the weather data and look at the first rows to identify the features:

```{r}
head(weatherAus) %>% gt()
```

\
In the next step we check out the summary statistics of the dataset and identify the numerical and categorical attributes:

```{r}
skim(weatherAus)
```

As we can see from the figure above, there are 7 **categorical** attributes and 16 **numerical** attributes.\
Before taking a deeper look on all other attributes, we first did a brief exploration of the target variable:

-   **MISSING VALUES**

```{r}
missingValues <- sum(is.na(weatherAus$RainTomorrow))
missingValues
```

-   **FREQUENCY DISTRIBUTION OF VALUES**

```{r}
  weatherAus %>% select(RainTomorrow) %>%
  count(RainTomorrow) %>% drop_na() %>%
  ggplot(., aes(RainTomorrow, n, fill=RainTomorrow)) +
  geom_col(width = 0.5)+
  labs(x = "RainTomorrow", y = "Count")+
  theme_minimal()
```

-   **RATIO OF FREQUENCY DISTRIBUTION**

```{r}
  weatherAus %>% select(RainTomorrow) %>%
  count(RainTomorrow) %>% drop_na() %>%
  ggplot(., aes(x="", n, fill = RainTomorrow)) +
  geom_bar(width = 1, size = 1, color = "white",stat = "identity") + coord_polar("y", start = 0) +
  geom_text(aes(label = paste0(round((n/145460)*100),"%")),
            position = position_stack(vjust = 0.5)) +
  theme_classic() +
  labs(x = NULL, y = NULL) +
  theme(axis.line = element_blank())
```

\
From the plots drawn above, we can clearly see that RainTomorrow has 2 categories of values: **Yes** and **No**. There are far more NEGATIVE values than POSITIVE.*"No"* and *"Yes"* appears 76% of time and 22% of time respectively after deleting all NA values from the attribute.

```{r}
  weatherAus %>% select(RainToday) %>%
  count(RainToday) %>% drop_na() %>%
  ggplot(., aes(x="", n, fill = RainToday)) +
  geom_bar(width = 1, size = 1, color = "white",stat = "identity") + coord_polar("y", start = 0) +
  geom_text(aes(label = paste0(round((n/145460)*100),"%")),
            position = position_stack(vjust = 0.5)) +
  theme_classic() +
  labs(x = NULL, y = NULL) +
  theme(axis.line = element_blank())
```

\

The variable **RainToday** has a very similar value distribution as the target variable. ...!!!!!!!!!!!!!!!!!!!!!

------------------------------------------------------------------------

### Categorical values

All together there are 6 categorical features + a Date column. In order to make the information about the date more useful, we decided to extract the year, the month and the day from the date into three separate columns.\
This is done here below:

```{r}
weatherAusNew <- weatherAus %>% mutate(
  Year = year(Date),
  Month = month(Date),
  Day = day(Date)
) %>% select(-Date)
as_tibble(weatherAusNew)
```

------------------------------------------------------------------------

### Numerical values

There are 16 numerical attributes in the raw dataset, after adding three columns for year, month and day, there are in total 19 numerical attributes. The main goal when analyzing numerical data is to find the outliers. Outliers are data information that differ significantly from other observations.

The most efficient way to detect outliers is to draw box plots:

> MinTemp

```{r}
 boxplot(weatherAusNew$MinTemp, col = "blue", border = "black")
```

> MaxTemp

```{r}
 boxplot(weatherAusNew$MaxTemp, col = "blue", border = "black")
```

> Rainfall

```{r}
 boxplot(weatherAusNew$Rainfall, col = "blue", border = "black")
```

> Evaporation

```{r}
 boxplot(weatherAusNew$Evaporation, col = "blue", border = "black")
```

> Sunshine

```{r}
 boxplot(weatherAusNew$Sunshine, col = "blue", border = "black")
```

> WindGustSpeed

```{r}
 boxplot(weatherAusNew$WindGustSpeed, col = "blue", border = "black")
```

> WindSpeed9am

```{r}
 boxplot(weatherAusNew$WindSpeed9am, col = "blue", border = "black")
```

> WindSpeed3pm

```{r}
 boxplot(weatherAusNew$WindSpeed3pm, col = "blue", border = "black")
```

> Humidity9am

```{r}
 boxplot(weatherAusNew$Humidity9am, col = "blue", border = "black")
```

> Humidity3pm

```{r}
 boxplot(weatherAusNew$Humidity3pm, col = "blue", border = "black")
```

> Pressure9am

```{r}
 boxplot(weatherAusNew$Pressure9am, col = "blue", border = "black")
```

> Pressure3pm

```{r}
 boxplot(weatherAusNew$Pressure3pm, col = "blue", border = "black")
```

> Cloud9am

```{r}
 boxplot(weatherAusNew$Cloud9am, col = "blue", border = "black")
```

> Cloud3pm

```{r}
 boxplot(weatherAusNew$Cloud3pm, col = "blue", border = "black")
```

> Temp9am

```{r}
 boxplot(weatherAusNew$Temp9am, col = "blue", border = "black")
```

> Temp3pm

```{r}
 boxplot(weatherAusNew$Temp3pm, col = "blue", border = "black")
```

> Year

```{r}
 boxplot(weatherAusNew$Year, col = "blue", border = "black")
```

> Month

```{r}
 boxplot(weatherAusNew$Month, col = "blue", border = "black")
```

> Day

```{r}
 boxplot(weatherAusNew$Day, col = "blue", border = "black")
```


### Multicollinearity

```{r}
#| label: fig-absences
#| fig-cap: "Correlation Heatmap"
weatherAusNew %>% select(where(is.numeric)) %>% model.matrix(~0+.,
              data=.) %>%
    cor(use="pairwise.complete.obs") %>% 
    ggcorrplot(show.diag = FALSE, type="full",
               lab=TRUE,legend.title = "Correlation" ,lab_size
               = 2,lab_col = "black" ,ggtheme =
                 ggplot2::theme_gray,
               colors = c("white","green","darkgreen"),
               outline.color = "black")

write.csv(weatherAusNew, file = "weatherNewToPython.csv", row.names = FALSE)
colnames(weatherAusNew)
```


#### Outliers

After drawing a boxplot for each numerical attribute in the dataset, we compared the mean of each column with the min/max value and we have noticed that that attributes **Rainfall**, **Evaporation**, **WindSpeed9am** and **WindSpeed3pm** might have a large number of outliers as there's a considerable difference between average value and max value. This also can be seen from their plots, as there is a huge amount of points (values) that differ from the average.

Let's remove the outliers values from the dataset:

```{r}
data <- weatherAusNew[,c(4,5,11,12)]
#rainfall
quartiles1 <- quantile(data$Rainfall, probs=c(.25,.75), na.rm = TRUE)
    IQR <- IQR(data$Rainfall)
    
    Lower <- quartiles[1] - 1.5*IQR
    Upper <- quartiles[2] + 1.5*IQR

dataNoOut1 <- subset(data, data$Rainfall > Lower & data$Rainfall< Upper )   

 boxplot(dataNoOut1$Rainfall, col = "blue", border = "black")

```

PYTHON NE DELA EN KURACCC

```{r}
#| output: false
# SETTING UP PYTHON ON RSTUDIO
library(reticulate)

virtualenv_create("my-python", python_version = "3.12")
use_virtualenv("my-python", required = TRUE)

virtualenv_install(envname = "my-python", "matplotlib",ignore_installed = FALSE, pip_options = character())

virtualenv_install(envname = "my-python", "numpy",ignore_installed = FALSE, pip_options = character())

virtualenv_install(envname = "my-python", "pandas",ignore_installed = FALSE, pip_options = character())

virtualenv_install(envname = "my-python", "seaborn",ignore_installed = FALSE, pip_options = character())

virtualenv_install(envname = "my-python", "scikit-learn",ignore_installed = FALSE, pip_options = character())
```


```{python}

import pandas as pd
import numpy as np
 
import seaborn as sns
from matplotlib import pyplot as plt


df = pd.read_csv('weatherNewToPython.csv')
```



------------------------------------------------------------------------

MAYBE SET THE **RAINTOMORROW** VALUES yes/no -\> 1/0!!!!! CHECK

look at function for deleting outliers, more analysis before modeling

## Modeling

After performing the Exploratory Data Analysis, we will proceed to the modeling part. In this phase we will implement three models: **Artificial Neural Network**, **Logistic Regression**, and **CatBoost**. 

Till now the analysis was made using code chunks performed in R langauge. For this implementation part the models will be implemented in Python.

### Artificial Neural Network

-   The ANN model derives from Biological neural networks that have the structure of the human brain. It contains neurons(nodes) interconnected to one another in various layers of the network. It consists of three layers: Input layer, Hidden layers (can be several of them) and Output layer. The input layer accepts inputs in several formats, the hidden layer is in-between the inputs and outputs and performs calculations to find hidden features and patterns. The output layer outputs the results of calculations.

    ![ANN layers](ANNPicture.png)

\

-   *ADVANTAGES*

    -   **Parallel processing**

    -   Information can produce **output** even with **inadequate data**

    -   **Success** proportional to **chosen instances**

-   *DISADVANTAGES*

    -   Depends on **hardware**

    -   ANNs work with **numerical data**

    -   Duration of network is **unknown**

-   **FUNCTIONING**

    Each input is multiplied by its corresponding weights (strength of interconnections between neurons). All weighted inputs are summarized inside the computing unit. Each neuron has its **bias,** which is added to the weighted sum to make it non-zero, so the total sum of weighted inputs can be from 0 to plus infinity. The maximum value is **benchmarked** to keep the response in the limits. This is performed in *TRANSFER FUNCTIONS.*

    *ACTIVATION FUNCTIONS* choose whether a node should fire or not. Only those who are *fired* make it to the output layer. Activation functions are distinctive depending on the task that is performed.

-   **FEED-BACK**

    -   Feed-back networks feed information back to itself

-   **FEED-FORWARD**

    -   Assessment of ouputs by reviewing its inputs

    -   Input –\> Neuron layer –\> Output

------------------------------------------------------------------------

```{python}


```

### CatBoost

-   Catboost is a machine learning algorithm that excels in classification and regression tasks. As a **gradient boosting** algorithm, it builds an ensemble of decision trees, where each tree corrects the errors of the previous ones. This iterative process enhances the model's accuracy and robustness.

-   **FUNCTIONING**

    Catboost follows the principles of gradient boosting but introduces several unique innovations:

    -   *Initialization:* The algorithm begins with a simple initial model, such as predicting the **mean value** for regression or **uniform probability** for classification.

    -   *Sequential Tree Building:* Decision trees are constructed one by one. Each tree is designed to minimize a specified loss function by addressing the errors from previous trees.

    -   *Model Update:* Newly built tree is added to the model, refining it.

    -   *Iteration:* The process repeats until the stopping criterion is met.

-   **Key Concepts:**

    -   [Gradient Boosting:]{.underline} Ensemble learning technique that combines weak prediction models, typically prediction trees, to form a strong predictive model. It iteratively adds new models to the ensemble.

    -   [Handling Categorical Features:]{.underline} It handles categorical features directly, without requiring extensive preprocessing. Very effective for real-world datasets that contain qualitative data, improving the performance.

    -   Learning Rate: The learning rate in CatBoost controls the step size at which the model learns during boosting. CatBoost automatically selects an optimal learning rate based on the dataset's features, striking a **balance** between **learning speed** and **model accuracy.** This helps achieving better performance with minimal tuning.

    ![Sequential Tree Building approach](CaBoostPicture.jpg)

------------------------------------------------------------------------

```{python}

```

### Logistic Regression

-   **Logistic Regression** is a supervised machine learning algorithm used for classification tasks where the goal is to predict the probability that an instance belongs to a given class or not. This model is used for binary classification where we use the sigmoid function, that takes input as independent variables and produces a probability value between 0 and 1.

-   **Key Concepts:**

    -   Logistic Regression predicts the output of a categorical dependent variable. In our case we have the target variable *"RainTomorrow"* that has values "yes" and "no".

    -   It gives the probabilistic values which is between 0 and 1.

    -   Instead of fitting a regression line, we fit an **S shaped** logistic function, which predicts two maximum values.

        ![Sigmoid Function](LogRegPicture.png)

\

-   [**Sigmoid function**]{style="color:red;"} is a mathematical function used to map the predicted values to probabilities. It maps any real value into another value within the range of 0 and 1. This range is called the **Threshold value**.

-   **Types of Logistic regression**

    -   [Binomial:]{.underline} there can be only two possible types of dependent variables, such as 0 and 1, Pass or Fail, etc.

    -   [Multinomial:]{.underline} there can be 3 or more possible unordered types of the dependent variable, such as "cat", "dogs", or "sheep".

    -   [Ordinal:]{.underline} there can be 3 or more possible ordered types of dependent variables, such as "low", "medium", "high".

```{python}
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

# Load dataset
#file_path = 'weatherNewToPython.csv'
#weather_data = pd.read_csv(file_path)
weather_data = pd.read_csv('weatherNewToPython.csv')

# Display the first few rows and summary information of the dataset
print(weather_data.head())
print(weather_data.info())

# Define numerical and categorical features
num_features = weather_data.select_dtypes(include=['float64']).columns.tolist()
cat_features = weather_data.select_dtypes(include=['object']).columns.tolist()

# Separate features and target
X = weather_data.drop(columns=['RainTomorrow'])
y = weather_data['RainTomorrow']

# Convert target variable to binary
y = y.map({'No': 0, 'Yes': 1})

# Impute missing values
num_imputer = SimpleImputer(strategy='mean')
cat_imputer = SimpleImputer(strategy='most_frequent')

X[num_features] = num_imputer.fit_transform(X[num_features])
X[cat_features] = cat_imputer.fit_transform(X[cat_features])

# One-hot encode categorical variables
X = pd.get_dummies(X, columns=cat_features, drop_first=True)

# Split the dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train logistic regression model
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
print(classification_report(y_test, y_pred))
```
